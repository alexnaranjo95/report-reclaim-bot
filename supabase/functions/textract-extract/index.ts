import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2.53.0';

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
};

const supabaseUrl = Deno.env.get('SUPABASE_URL');
const supabaseServiceKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY');
const awsAccessKeyId = Deno.env.get('AWS_ACCESS_KEY_ID');
const awsSecretAccessKey = Deno.env.get('AWS_SECRET_ACCESS_KEY');
const awsRegion = Deno.env.get('AWS_REGION') || 'us-east-1';
const googleCloudVisionApiKey = Deno.env.get('GOOGLE_CLOUD_VISION_API_KEY');
const googleCloudProjectId = Deno.env.get('GOOGLE_CLOUD_PROJECT_ID');
const googleCloudServiceAccountKey = Deno.env.get('GOOGLE_CLOUD_SERVICE_ACCOUNT_KEY');

serve(async (req) => {
  console.log(`üìã Request method: ${req.method}, URL: ${req.url}`);
  
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders });
  }

  let reportId = null;
  
  try {
    // Validate environment variables
    if (!supabaseUrl || !supabaseServiceKey) {
      throw new Error('Supabase configuration missing');
    }

    const requestBody = await req.json();
    reportId = requestBody.reportId;
    const filePath = requestBody.filePath;
    
    console.log(`üöÄ Starting parallel text extraction for report ${reportId}, file: ${filePath}`);

    const supabase = createClient(supabaseUrl, supabaseServiceKey);

    // Update status to processing
    await supabase
      .from('credit_reports')
      .update({ 
        extraction_status: 'processing',
        consolidation_status: 'processing',
        processing_errors: null,
        updated_at: new Date().toISOString()
      })
      .eq('id', reportId);

    // Download the PDF from storage
    const { data: fileData, error: downloadError } = await supabase.storage
      .from('credit-reports')
      .download(filePath);

    if (downloadError) {
      throw new Error(`Failed to download PDF: ${downloadError.message}`);
    }

    const arrayBuffer = await fileData.arrayBuffer();
    console.log('üìÑ Downloaded PDF, size:', arrayBuffer.byteLength);

    // PARALLEL EXTRACTION: Run all OCR services simultaneously
    const extractionPromises = [];
    const extractionMethods = [];

    // Google Document AI
    if (googleCloudProjectId && googleCloudServiceAccountKey) {
      console.log('üîÑ Queuing Google Document AI extraction...');
      extractionMethods.push('google-document-ai');
      extractionPromises.push(
        extractWithGoogleDocumentAI(arrayBuffer)
          .catch(error => ({ error: error.message, text: '', method: 'google-document-ai' }))
      );
    }

    // Google Cloud Vision  
    if (googleCloudVisionApiKey) {
      console.log('üîÑ Queuing Google Cloud Vision extraction...');
      extractionMethods.push('google-vision');
      extractionPromises.push(
        extractWithGoogleVision(arrayBuffer)
          .catch(error => ({ error: error.message, text: '', method: 'google-vision' }))
      );
    }

    // AWS Textract
    if (awsAccessKeyId && awsSecretAccessKey) {
      console.log('üîÑ Queuing AWS Textract extraction...');
      extractionMethods.push('textract');
      extractionPromises.push(
        extractWithTextract(arrayBuffer)
          .catch(error => ({ error: error.message, text: '', method: 'textract' }))
      );
    }

    // Fallback extraction (always run)
    console.log('üîÑ Queuing fallback extraction...');
    extractionMethods.push('fallback');
    extractionPromises.push(
      fallbackTextExtraction(arrayBuffer)
        .catch(error => ({ error: error.message, text: '', method: 'fallback' }))
    );

    console.log(`‚ö° Running ${extractionPromises.length} extraction methods in parallel...`);
    
    // Execute all extractions in parallel
    const startTime = Date.now();
    const extractionResults = await Promise.all(extractionPromises);
    const totalTime = Date.now() - startTime;

    console.log(`‚úÖ All extractions completed in ${totalTime}ms`);

    // Process and store individual extraction results
    const validResults = [];
    
    for (let i = 0; i < extractionResults.length; i++) {
      const result = extractionResults[i];
      const method = extractionMethods[i];
      
      let extractedText = '';
      let hasError = false;
      
      if (result.error) {
        console.log(`‚ùå ${method} failed:`, result.error);
        hasError = true;
      } else if (typeof result === 'string') {
        extractedText = result;
      } else if (result.text) {
        extractedText = result.text;
      }

      const characterCount = extractedText.length;
      const wordCount = extractedText.split(/\s+/).filter(w => w.length > 0).length;
      const confidence = calculateConfidenceScore(extractedText, method);
      const hasStructuredData = hasExtractableData(extractedText);

      // Store each extraction result
      await supabase
        .from('extraction_results')
        .insert({
          report_id: reportId,
          extraction_method: method,
          extracted_text: extractedText || null,
          processing_time_ms: Math.round(totalTime / extractionPromises.length),
          character_count: characterCount,
          word_count: wordCount,
          confidence_score: confidence,
          has_structured_data: hasStructuredData,
          extraction_metadata: {
            hasError,
            errorMessage: result.error || null,
            processingTime: totalTime
          }
        });

      if (!hasError && extractedText.length > 100) {
        validResults.push({
          method,
          text: extractedText,
          confidence,
          characterCount,
          wordCount,
          hasStructuredData
        });
      }

      console.log(`üìä ${method}: ${characterCount} chars, confidence: ${confidence}, structured: ${hasStructuredData}`);
    }

    if (validResults.length === 0) {
      throw new Error('All extraction methods failed to produce valid text');
    }

    // CONSOLIDATION: Choose the best result and create consolidated text
    const consolidationResult = await consolidateExtractionResults(validResults, reportId, supabase);
    
    console.log(`üéØ Selected primary method: ${consolidationResult.primaryMethod} with confidence: ${consolidationResult.confidence}`);

    // Update the database with consolidated result
    const { error: updateError } = await supabase
      .from('credit_reports')
      .update({
        raw_text: consolidationResult.consolidatedText,
        extraction_status: 'completed',
        consolidation_status: 'completed',
        consolidation_confidence: consolidationResult.confidence,
        primary_extraction_method: consolidationResult.primaryMethod,
        updated_at: new Date().toISOString()
      })
      .eq('id', reportId);

    if (updateError) {
      console.error('Failed to update database:', updateError);
      throw new Error(`Database update failed: ${updateError.message}`);
    }

    console.log(`‚úÖ Parallel extraction completed successfully with consolidation`);

    return new Response(JSON.stringify({ 
      success: true,
      extractedText: consolidationResult.consolidatedText,
      textLength: consolidationResult.consolidatedText.length,
      primaryMethod: consolidationResult.primaryMethod,
      consolidationConfidence: consolidationResult.confidence,
      extractionResults: validResults.map(r => ({
        method: r.method,
        confidence: r.confidence,
        characterCount: r.characterCount,
        hasStructuredData: r.hasStructuredData
      })),
      hasValidText: consolidationResult.consolidatedText.length > 100
    }), {
      headers: { ...corsHeaders, 'Content-Type': 'application/json' },
    });

  } catch (error) {
    console.error('‚ùå Parallel text extraction error:', error);
    
    // Update status to failed if we have a reportId
    if (reportId && supabaseUrl && supabaseServiceKey) {
      try {
        const supabase = createClient(supabaseUrl, supabaseServiceKey);
        await supabase
          .from('credit_reports')
          .update({
            extraction_status: 'failed',
            consolidation_status: 'failed',
            processing_errors: `${error.message} | Stack: ${error.stack?.substring(0, 500) || 'No stack trace'}`,
            updated_at: new Date().toISOString()
          })
          .eq('id', reportId);
      } catch (updateError) {
        console.error('Failed to update error status:', updateError);
      }
    }

    return new Response(JSON.stringify({ 
      success: false,
      error: error.message,
      details: error.stack?.substring(0, 500) || 'No additional details'
    }), {
      status: 500,
      headers: { ...corsHeaders, 'Content-Type': 'application/json' },
    });
  }
});

// Consolidation logic to choose best result and create consolidated text
async function consolidateExtractionResults(
  results: Array<{
    method: string;
    text: string;
    confidence: number;
    characterCount: number;
    wordCount: number;
    hasStructuredData: boolean;
  }>,
  reportId: string,
  supabase: any
) {
  console.log('üîÑ Starting consolidation process...');
  
  // Sort by confidence score (descending)
  const sortedResults = [...results].sort((a, b) => b.confidence - a.confidence);
  
  // Choose primary source (highest confidence with structured data preference)
  let primaryResult = sortedResults[0];
  
  // Prefer results with structured data if confidence is close
  for (const result of sortedResults) {
    if (result.hasStructuredData && 
        (result.confidence >= primaryResult.confidence - 0.1)) {
      primaryResult = result;
      break;
    }
  }

  // Simple consolidation strategy: use primary result as base
  let consolidatedText = primaryResult.text;
  let consolidationStrategy = 'highest_confidence';
  let conflictCount = 0;
  
  // For advanced consolidation, we could merge data from multiple sources
  // but for now, we'll use the single best result
  
  // Calculate overall confidence
  const overallConfidence = Math.min(0.99, 
    primaryResult.confidence * 0.8 + 
    (results.length > 1 ? 0.1 : 0) + 
    (primaryResult.hasStructuredData ? 0.1 : 0)
  );

  // Store consolidation metadata
  await supabase
    .from('consolidation_metadata')
    .insert({
      report_id: reportId,
      primary_source: primaryResult.method,
      consolidation_strategy,
      confidence_level: overallConfidence,
      field_sources: {
        primary_text: primaryResult.method,
        total_sources: results.length,
        methods_used: results.map(r => r.method)
      },
      conflict_count: conflictCount,
      requires_human_review: overallConfidence < 0.7,
      consolidation_notes: `Selected ${primaryResult.method} with ${results.length} total sources. Confidence: ${overallConfidence}`
    });

  return {
    consolidatedText,
    primaryMethod: primaryResult.method,
    confidence: overallConfidence,
    strategy: consolidationStrategy
  };
}

// Calculate confidence score based on text quality and extraction method
function calculateConfidenceScore(text: string, method: string): number {
  if (!text || text.length < 50) return 0.0;
  
  let baseScore = 0.5;
  
  // Method-based scoring
  switch (method) {
    case 'google-document-ai':
      baseScore = 0.9;
      break;
    case 'google-vision':
      baseScore = 0.8;
      break;
    case 'textract':
      baseScore = 0.7;
      break;
    case 'fallback':
      baseScore = 0.3;
      break;
  }
  
  // Text quality adjustments
  const lengthScore = Math.min(0.3, text.length / 10000);
  const alphaRatio = (text.match(/[a-zA-Z]/g) || []).length / text.length;
  const creditKeywords = (text.match(/credit|report|account|balance|payment|inquiry/gi) || []).length;
  
  const qualityScore = lengthScore + (alphaRatio * 0.2) + Math.min(0.2, creditKeywords / 50);
  
  return Math.min(0.99, baseScore + qualityScore);
}

// Check if text contains extractable credit report data
function hasExtractableData(text: string): boolean {
  if (!text || text.length < 200) return false;
  
  const creditIndicators = [
    /credit\s*report/i,
    /personal\s*information/i,
    /account\s*number/i,
    /payment\s*history/i,
    /credit\s*score/i,
    /inquiry|inquiries/i,
    /creditor/i,
    /balance/i
  ];
  
  let matches = 0;
  for (const pattern of creditIndicators) {
    if (pattern.test(text)) matches++;
  }
  
  return matches >= 3;
}

// Google Document AI extraction function  
async function extractWithGoogleDocumentAI(pdfBuffer: ArrayBuffer): Promise<string> {
  const serviceAccountKey = JSON.parse(Deno.env.get('GOOGLE_CLOUD_SERVICE_ACCOUNT_KEY') || '{}');
  const projectId = Deno.env.get('GOOGLE_CLOUD_PROJECT_ID');
  
  if (!serviceAccountKey.client_email || !serviceAccountKey.private_key) {
    throw new Error('Invalid Google Cloud service account configuration');
  }
  
  // Convert PDF to base64
  const base64Data = btoa(String.fromCharCode(...new Uint8Array(pdfBuffer)));
  
  // Get access token using service account
  const tokenResponse = await fetch('https://oauth2.googleapis.com/token', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/x-www-form-urlencoded',
    },
    body: new URLSearchParams({
      grant_type: 'urn:ietf:params:oauth:grant-type:jwt-bearer',
      assertion: await createJWT(serviceAccountKey),
    }),
  });
  
  if (!tokenResponse.ok) {
    const errorText = await tokenResponse.text();
    throw new Error(`Token request failed: ${tokenResponse.status} - ${errorText}`);
  }
  
  const tokenData = await tokenResponse.json();
  const accessToken = tokenData.access_token;
  
  // Process document with Document AI
  const documentAIResponse = await fetch(
    `https://documentai.googleapis.com/v1/projects/${projectId}/locations/us/processors/general:process`,
    {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${accessToken}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        rawDocument: {
          content: base64Data,
          mimeType: 'application/pdf'
        }
      }),
    }
  );
  
  if (!documentAIResponse.ok) {
    const errorText = await documentAIResponse.text();
    throw new Error(`Document AI request failed: ${documentAIResponse.status} - ${errorText}`);
  }
  
  const result = await documentAIResponse.json();
  return result.document?.text || '';
}

// Google Cloud Vision API extraction function
async function extractWithGoogleVision(pdfBuffer: ArrayBuffer): Promise<string> {
  const apiKey = Deno.env.get('GOOGLE_CLOUD_VISION_API_KEY');
  
  // NOTE: Vision API doesn't directly support PDFs, so this is a simplified approach
  // In production, you'd want to convert PDF to images first
  const base64Data = btoa(String.fromCharCode(...new Uint8Array(pdfBuffer)));
  
  const response = await fetch(`https://vision.googleapis.com/v1/images:annotate?key=${apiKey}`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      requests: [{
        image: {
          content: base64Data
        },
        features: [{
          type: 'TEXT_DETECTION',
          maxResults: 1
        }]
      }]
    }),
  });
  
  if (!response.ok) {
    const errorText = await response.text();
    throw new Error(`Vision API error: ${response.status} - ${errorText}`);
  }
  
  const result = await response.json();
  
  // Check for API errors in response
  if (result.responses?.[0]?.error) {
    throw new Error(`Vision API processing error: ${result.responses[0].error.message}`);
  }
  
  return result.responses?.[0]?.fullTextAnnotation?.text || '';
}

// Create JWT for Google service account authentication
async function createJWT(serviceAccount: any): Promise<string> {
  const now = Math.floor(Date.now() / 1000);
  const header = {
    alg: 'RS256',
    typ: 'JWT'
  };
  
  const payload = {
    iss: serviceAccount.client_email,
    scope: 'https://www.googleapis.com/auth/cloud-platform',
    aud: 'https://oauth2.googleapis.com/token',
    exp: now + 3600,
    iat: now
  };
  
  const encodedHeader = btoa(JSON.stringify(header)).replace(/[+/=]/g, (m) => ({'+': '-', '/': '_', '=': ''}[m] || m));
  const encodedPayload = btoa(JSON.stringify(payload)).replace(/[+/=]/g, (m) => ({'+': '-', '/': '_', '=': ''}[m] || m));
  
  const data = `${encodedHeader}.${encodedPayload}`;
  
  try {
    // Clean and format the private key
    let privateKeyPem = serviceAccount.private_key.replace(/\\n/g, '\n');
    
    // Ensure proper PEM format
    if (!privateKeyPem.includes('-----BEGIN PRIVATE KEY-----')) {
      throw new Error('Invalid private key format');
    }
    
    // Import the private key
    const privateKey = await crypto.subtle.importKey(
      'pkcs8',
      new TextEncoder().encode(privateKeyPem),
      {
        name: 'RSASSA-PKCS1-v1_5',
        hash: 'SHA-256',
      },
      false,
      ['sign']
    );
    
    // Sign the data
    const signature = await crypto.subtle.sign(
      'RSASSA-PKCS1-v1_5',
      privateKey,
      new TextEncoder().encode(data)
    );
    
    const encodedSignature = btoa(String.fromCharCode(...new Uint8Array(signature)))
      .replace(/[+/=]/g, (m) => ({'+': '-', '/': '_', '=': ''}[m] || m));
    
    return `${data}.${encodedSignature}`;
  } catch (error) {
    throw new Error(`JWT creation failed: ${error.message}`);
  }
}

// AWS Textract extraction function with proper signature
async function extractWithTextract(pdfBuffer: ArrayBuffer): Promise<string> {
  const accessKeyId = Deno.env.get('AWS_ACCESS_KEY_ID');
  const secretAccessKey = Deno.env.get('AWS_SECRET_ACCESS_KEY');
  const region = Deno.env.get('AWS_REGION') || 'us-east-1';
  
  if (!accessKeyId || !secretAccessKey) {
    throw new Error('AWS credentials not configured');
  }
  
  // For this implementation, we'll use a simplified approach
  // In production, you'd implement full AWS Signature Version 4
  const textractUrl = `https://textract.${region}.amazonaws.com/`;
  
  const date = new Date();
  const dateString = date.toISOString().substr(0, 10).replace(/-/g, '');
  const amzDate = date.toISOString().replace(/[-:]/g, '').substr(0, 15) + 'Z';
  
  // Convert ArrayBuffer to base64 for Textract
  const base64Data = btoa(String.fromCharCode(...new Uint8Array(pdfBuffer)));
  
  const payload = JSON.stringify({
    Document: {
      Bytes: base64Data
    }
  });
  
  const headers = {
    'Content-Type': 'application/x-amz-json-1.1',
    'X-Amz-Target': 'Textract.DetectDocumentText',
    'X-Amz-Date': amzDate,
    // Note: This is a simplified implementation
    // Production code would need proper AWS Signature Version 4
    'Authorization': `AWS4-HMAC-SHA256 Credential=${accessKeyId}/${dateString}/${region}/textract/aws4_request, SignedHeaders=content-type;host;x-amz-date;x-amz-target, Signature=placeholder`
  };
  
  // For now, we'll simulate a simple extraction since implementing full AWS sig v4 is complex
  // In production, you'd use the AWS SDK or implement proper signatures
  throw new Error('AWS Textract integration requires full signature implementation');
}

// Fallback text extraction using basic PDF parsing
async function fallbackTextExtraction(arrayBuffer: ArrayBuffer): Promise<string> {
  console.log('üîÑ Attempting basic PDF text extraction...');
  
  try {
    const uint8Array = new Uint8Array(arrayBuffer);
    const decoder = new TextDecoder('utf-8', { ignoreBOM: true, fatal: false });
    const pdfContent = decoder.decode(uint8Array);
    
    let extractedText = '';
    
    // Extract readable text between stream objects
    const textMatches = pdfContent.match(/\((.*?)\)/g);
    if (textMatches) {
      extractedText = textMatches
        .map(match => match.slice(1, -1)) // Remove parentheses
        .filter(text => text.length > 2 && !/^[\s\n\r]*$/.test(text)) // Filter out short/empty strings
        .join(' ');
    }

    // If no text found, try extracting from Tj operators
    if (extractedText.length < 100) {
      const tjMatches = pdfContent.match(/\[(.*?)\]\s*TJ/g);
      if (tjMatches) {
        extractedText = tjMatches
          .map(match => match.replace(/\[(.*?)\]\s*TJ/, '$1'))
          .filter(text => text.length > 2)
          .join(' ');
      }
    }

    // Try BT/ET text extraction blocks
    if (extractedText.length < 100) {
      const btMatches = pdfContent.match(/BT\s*(.*?)\s*ET/gs);
      if (btMatches) {
        extractedText = btMatches
          .map(match => {
            // Extract text from Tj operations within BT/ET blocks
            const tjOps = match.match(/\((.*?)\)\s*Tj/g);
            return tjOps ? tjOps.map(tj => tj.replace(/\((.*?)\)\s*Tj/, '$1')).join(' ') : '';
          })
          .filter(text => text.length > 2)
          .join(' ');
      }
    }

    // Clean up the extracted text
    extractedText = extractedText
      .replace(/\\n/g, '\n')
      .replace(/\\r/g, '\r')
      .replace(/\\t/g, '\t')
      .replace(/\\\\/g, '\\')
      .replace(/\s+/g, ' ')
      .trim();
    
    console.log(`‚úÖ Basic PDF extraction completed: ${extractedText.length} characters`);
    
    if (extractedText.length < 100) {
      throw new Error('Insufficient text extracted from PDF - likely an image-based document');
    }
    
    return extractedText;
    
  } catch (error) {
    console.error('‚ùå Basic PDF extraction failed:', error);
    throw error;
  }
}